[{"content":"Differentiating Vector Fields on Manifolds Affine Connections Notion of derivative for vector fields on manifolds is called a connection , traditionally denoted by $\\nabla$ (\u0026quot;nabla\u0026quot;). Given a tangent vector $u \\in T_x\\mathcal{M}$ and a vector field $V$, $\\nabla_uV$ is the derivative of $V$ at $x$ along $u$. Formally, we should write $\\nabla_{(x,u)} V$ where the base point $x$ is typically clear from context.\nNote that we do not need a Riemannian metric yet.\nDefinition 1. A connection on a manifold $M$ is an operator $$\\nabla \\colon \\operatorname{T}_{}{\\mathcal{M}} \\times \\mathfrak{X}(\\mathcal{M}) \\to \\operatorname{T}_{}{\\mathcal{M}}: (u, V) \\mapsto \\nabla_u V$$ where:\n$\\operatorname{T}_{}{\\mathcal{M}}$ is the tangent vector space\n$\\mathfrak{X}(\\mathcal{M})$ denotes smooth vector fields on $\\mathcal{M}$\nThis operator must satisfy four properties for all $u,w \\in \\operatorname{T}_{}{\\mathcal{M}}$, $U,V,W \\in \\mathfrak{X}(\\mathcal{M})$, $a,b \\in \\mathbb{R}$, and $f \\in C^\\infty(\\mathcal{M})$:\nSmoothness: $(\\nabla_UV)(x) \\stackrel{\\Delta}{=}\\nabla_{U(x)}V$ defines a smooth vector field $\\nabla_UV$;\nLinearity in $u$: $\\nabla_{au + bw}V = a\\nabla_uV + b\\nabla_wV$;\nLinearity in $V$: $\\nabla_u(aV + bW) = a\\nabla_uV + b\\nabla_uW$;\nLeibniz rule: $\\nabla_u(fV) = \\operatorname{D}{f}(x)[u]\\cdot V(x) + f(x)\\nabla_{u}{V}$.\nThe field $\\nabla_U V$ is called the covariant derivative of $V$ along $U$ with respect to $\\nabla$.\nTheorem 2. Let $\\mathcal{M}$ be an embedded submanifold of a Euclidean space $\\mathcal{E}$. The operator $\\nabla$ defined by $$\\nabla_{u}{V} = \\operatorname{Proj}_{x} {\\left(\\operatorname{D}{\\bar V}(x)[u]\\right)} \\tag{Eq. 1}$$ is a connection on $\\mathcal{M}$, where $\\operatorname{Proj}_{x}$ is the projector from $\\mathcal{E}$ to the tangent vector space $\\operatorname{T} _x(\\mathcal{M})$.\nProof. Let $\\mathcal{M}$ be an embedded submanifold of a Euclidean space $\\mathcal{E}$ by $\\bar \\nabla$. Then $$\\nabla_u V = \\operatorname{Proj}_{x} {\\left(\\operatorname{D}{\\bar V}(x)[u]\\right)} = \\operatorname{Proj}_{x} {\\left(\\bar\\nabla_{u}{\\bar V}\\right)}.$$ Projection is a linear operation which maintains the properties (linearities of $u, V$) of connections $\\nabla_u V$, hence $\\operatorname{Proj}_{x} {\\left(\\operatorname{D}{\\bar V}(x)[u]\\right)}$ is a connection on $\\mathcal{M}$.\nDetails:\nIf $M$ is a submanifold of $\\mathcal{E}$, the claim is clear since $\\operatorname{Proj}_x$ is identity and we can take $\\bar V = V$. Suppose $M$ is a manifold not open in $\\mathcal{E}$. Consider $U,V,W \\in \\mathfrak{X}(\\mathcal{M})$ together with smooth extensions $\\bar U, \\bar V, \\bar W \\in \\mathfrak{X}(O)$ defined on a neighborhood $O$ of $\\mathcal{M}$. $\\bar \\nabla$ is a connection on $O$ since $O$ is an open submanifold of $E$. Consider $a, b \\in \\mathbb{R}$ and $u, w\\in \\operatorname{T} _x \\mathcal{M}$. Then, Since projection is a linear operation, $\\nabla _u V = \\operatorname{Proj} _{x} {\\left(\\bar\\nabla _{u}{\\bar V}\\right)}$ is a smooth vector field; $$\\begin{aligned}\\nabla _{au+bw} V \u0026= \\operatorname{Proj}_x (\\bar \\nabla _{au+bw} \\bar V) \\\\ \u0026= \\operatorname{Proj}_x (a\\bar \\nabla _{u} \\bar V + b \\bar \\nabla _{w} \\bar V) \\\\ \u0026= a \\nabla _{u} V + b \\nabla _{w} V \\end{aligned}$$ $$\\begin{aligned}\\nabla _{u} (aV+bW) \u0026= \\operatorname{Proj}_x \\left(\\bar \\nabla _{u} (a\\bar V+b\\bar W)\\right) \\\\ \u0026= \\operatorname{Proj}_x (a\\bar \\nabla _{u} \\bar V + b \\bar \\nabla _{u} \\bar W) \\\\ \u0026= a \\nabla _{u} V + b \\nabla _{u} W \\end{aligned}$$ To verify the Leibniz rule, consider an arbitrary $f\\in \\mathfrak{F}(\\mathcal{M})$ and smooth extension $\\bar f \\in \\mathfrak{F}(O)$. $$\\begin{aligned}\\nabla _{u} (fV) \u0026= \\operatorname{Proj}_x \\left(\\bar \\nabla _{u} (\\bar f \\bar V)\\right) \\\\ \u0026= \\operatorname{Proj}_x (\\operatorname{D} \\bar f (x) [u] \\cdot \\bar V(x) + \\bar f(x) \\bar \\nabla _{u} \\bar V) \\\\ \u0026= \\operatorname{D} f (x) [u] \\cdot V(x) + f(x) \\nabla _{u} V \\end{aligned}$$ Hence, $\\nabla _{U} V$ with $\\nabla _{u} V= \\operatorname{Proj} _{x} {\\left(\\operatorname{D}{\\bar V}(x)[u]\\right)}$ is a connection on $\\mathcal{M}$. $\\square$\nProposition 3. Let $\\mathcal{M}$ be a manifold with arbitrary connection $\\nabla$. Given a smooth vector field $V \\in \\mathfrak{X}(\\mathcal{M})$ and a point $x \\in \\mathcal{M}$, if $V(x) = 0$ then $\\nabla_u V=\\operatorname{D}{V}(x)[u]$ for all $u\\in \\operatorname{T}_{x}{\\mathcal{M}}$. In particular, $\\operatorname{D}{V}(x)[u]$ is tangent at $x$.\nRiemannian Connections Definition 4. For $U, V \\in \\mathfrak{X}(\\mathcal{M})$ and $f \\in \\mathfrak{F}(\\mathcal{U})$ with $\\mathcal{U}$ open in $\\mathcal{M}$, define:\n$Uf \\in \\mathfrak{F}(\\mathcal{U})$ such that $(Uf)(x) = \\operatorname{D}{f(}(x)[U(x)]$;\n$[U, V]: \\mathfrak{F}(\\mathcal{U})\\to \\mathfrak{F}(\\mathcal{U})$ such that $[U,V]f = U(Vf) - V(Uf)$;\n$\\langle U, V\\rangle \\in \\mathfrak{F}(\\mathcal{M})$ such that $\\langle U, V\\rangle(x) = \\langle U(x), V(x)\\rangle_x$.\nThe notation $Uf$ captures the action of a smooth vector field $U$ on a smooth function $f$ through derivation, transforming $f$ into another smooth function. The commutator $[U,V]$ of such action is called the Lie bracket. Even in linear spaces $[U, V ]f$ is nonzero in general. Notice that $Uf = \\langle \\operatorname{grad}f, U\\rangle$ owing to the definitions of $Uf, \\langle V, U\\rangle$ and $\\operatorname{grad} f$.\nTheorem 5. On a Riemannian manifold $\\mathcal{M}$, there exists a unique connection $\\nabla$ which satisfies two additional properties for all $U,V,W \\in \\mathfrak{X}(\\mathcal{M})$:\nSymmetry: $[U, V] f = (\\nabla_{U}{V} -\\nabla_{V}{U}) f$ for all $f \\in \\mathfrak{F}(\\mathcal{M})$;\nCompatibility with the metric: $U\\langle V,W\\rangle = \\langle\\nabla_{U}{V},W\\rangle+\\langle V,\\nabla_{U}{W}\\rangle$.\nThis connection is called the Levi-Civita or Riemannian connection.\nTheorem 6. The Riemannian connection on a Euclidean space $\\mathcal{E}$ with any Euclidean metric $\\langle \\cdot, \\cdot \\rangle$ is $\\nabla_{u}{V} = \\operatorname{D}{V}(x)[u]$: the canonical Euclidean connection.\nTheorem 7. Let $\\mathcal{M}$ be an embedded submanifold of a Euclidean space $\\mathcal{E}$. The connection $\\nabla_{}{}$ defined by Eq. 1 is symmetric on $\\mathcal{M}$.\nTheorem 8. Let $M$ be a Riemannian submanifold of a Euclidean space. The connection $\\nabla_{}{}$ defined by Eq. 1 is the Riemannian connection on $\\mathcal{M}$.\nProposition 9. Let $U,V$ be two smooth vector fields on a manifold $\\mathcal{M}$. There exists a unique smooth vector field $W$ on $\\mathcal{M}$ such that $[U,V]f = Wf$ for all $f \\in \\mathfrak{F}(\\mathcal{M})$. Therefore, we identify $[U,V]$ with that smooth vector field. Explicitly, if $\\nabla_{}{}$ is any symmetric connection, then $[U,V ] = \\nabla_{U}{V} - \\nabla_{V}{U}$.\nRiemannian Hessians Definition 10. Let $\\mathcal{M}$ be a Riemannian manifold with its Riemannian con nection $\\nabla_{}{}$. The Riemannian Hessian of $f\\in \\mathfrak{F}(\\mathcal{M})$ at $x\\in \\mathcal{M}$ is the linear map $\\operatorname{Hess} f(x): \\operatorname{T} _{x}{\\mathcal{M}} \\to \\operatorname{T} _{x}{\\mathcal{M}}$ defined as follows: $$\\operatorname{Hess}f(x)[u] = \\nabla_{u}{\\operatorname{grad} f}.$$Equivalently, $\\operatorname{Hess}f$ maps $\\mathfrak{X}(\\mathcal{M})$ to $\\mathfrak{X}(\\mathcal{M})$ as $\\operatorname{Hess}f[U] = \\nabla_{U}{\\operatorname{grad} f}$.\nProposition 11. The Riemannian Hessian is self-adjoint with respect to the Riemannian metric. That is, for all $x\\in \\mathcal{M}$ and $u, v\\in \\operatorname{T} _{x}{\\mathcal{M}}$, $\\langle \\operatorname{Hess} f(x)[u], v\\rangle _x = \\langle u, \\operatorname{Hess}f(x)[v]\\rangle _x$.\nCorollary 12. Let $\\mathcal{M}$ be a Riemannian submanifold of a Euclidean space. Consider a smooth function $f : \\mathcal{M}\\to \\mathbb{R}$. Let $\\bar G$ be a smooth extension of $\\operatorname{grad}f$\u0026mdash; that is, $\\bar G$ is any smooth vector field defined on a neighborhood of $\\mathcal{M}$ in the embedding space such that $\\bar G(x) = \\operatorname{grad}f(x)$ for all $x \\in \\mathcal{M}$. Then, $\\operatorname{Hess}f(x)[u] = \\operatorname{Proj}_{x}{\\left(\\operatorname{D}\\bar G(x)[u]\\right)}$.\nConnections as Pointwise Derivatives* Definition 13. A connection on a manifold $\\mathcal{M}$ is an operator $$\\nabla \\colon \\mathfrak{X}(\\mathcal{M}) \\times \\mathfrak{X}(\\mathcal{M}) \\to \\mathfrak{X}(\\mathcal{M}): (U,V) \\mapsto \\nabla_{U}{V}$$ which has three properties for all $U ,V , W \\in \\mathfrak{X}(\\mathcal{M})$, $f, g \\in \\mathfrak{F}(\\mathcal{M})$ and $a, b \\in \\mathbb{R}$:\n$\\mathfrak{F}(\\mathcal{M})$-linearity in $U$: $\\nabla_{fU+gW}{V} = f\\nabla_{U}{V} + g\\nabla_{W}{V}$;\n$\\mathbb{R}$-linearity in $V$: $\\nabla_{U}{(aV+bW)} = a\\nabla_{U}{V} + b\\nabla_{U}{W}$; and\nLeibniz rule: $\\nabla_{U}{(fV)} = (Uf)V + f \\nabla_{U}{V}$.\nThe field $\\nabla_{U}{V}$ is the covariant derivative of $V$ along $U$ with respect to $\\nabla_{}{}$.\nProposition 14. For any connection $\\nabla_{}{}$ and smooth vector fields $U,V$ on a manifold $\\mathcal{M}$, the vector field $\\nabla_{U}{V}$ at $x$ depends on $U$ only through $U(x)$.\nLemma 15. Given any real numbers $0 \u0026lt; r_1 \u0026lt; r_2$ and any point $x$ in a Euclidean space $\\mathcal{E}$ with norm $\\parallel \\cdot \\parallel$, there exists a smooth function $b: \\mathcal{E}\\to \\mathbb{R}$ such that\n$b(y) = 1$ if $\\parallel y - x \\parallel \\leq r_1$;\n$b(y) = 0$ if $\\parallel y - x \\parallel \\geq r_2$; and\n$b(y) \\in (0,1)$ if $\\parallel y - x \\parallel \\in (r_1, r_2)$.\nUsing bump functions, we can show that $(\\nabla_UV)(x)$ depends on $U$ and $V$ only through their values in a neighborhood around $x$. This is the object of the two following lemmas.\nLemma 16. Let $V_1, V_2$ be smooth vector fields on a manifold $M$ equipped with a connection $\\nabla_{}{}$. If $V_1 \\mid_\\mathcal{U}= V_2 \\mid_\\mathcal{U}$ on some open set $\\mathcal{U}$ of $\\mathcal{M}$, then $(\\nabla_{U}{V_1})\\mid_\\mathcal{U}= (\\nabla_{U}{V_2})\\mid_\\mathcal{U}$ for all $U \\in \\mathfrak{X}(\\mathcal{M})$.\nLemma 17. Let $U_1, U_2$ be smooth vector fields on a manifold $\\mathcal{M}$ equipped with a connection $\\nabla_{}{}$. If $U_1\\mid_\\mathcal{U}= U_2\\mid_\\mathcal{U}$ on some open set $\\mathcal{U}$ of $\\mathcal{M}$, then $(\\nabla_{U_1}{V})\\mid_\\mathcal{U}= (\\nabla_{U_2}{V})\\mid_\\mathcal{U}$ for all $V \\in \\mathfrak{X}(\\mathcal{M})$.\nLemma 18. Let $U$ be a neighborhood of a point $x$ on a manifold $\\mathcal{M}$. Given a smooth function $f\\in \\mathfrak{F}(\\mathcal{U})$, there exists a smooth function $g\\in \\mathfrak{F}(\\mathcal{M})$ and a neighborhood $\\mathcal{U}\u0026rsquo; \\subseteq \\mathcal{U}$ of $x$ such that $g\\mid \\mathcal{U}\u0026rsquo; = f\\mid\\mathcal{U}\u0026rsquo;$.\nLemma 19. Let $U$ be a neighborhood of a point $x$ on a manifold $\\mathcal{M}$. Given a smooth vector field $U \\in \\mathfrak{X}(\\mathcal{U})$, there exists a smooth vector field $V \\in \\mathfrak{X}(\\mathcal{M})$ and a neighborhood $\\mathcal{U}\u0026rsquo; \\subseteq \\mathcal{U}$ of $x$ such that $V\\mid_{\\mathcal{U}\u0026rsquo;} = U\\mid_{\\mathcal{U}\u0026rsquo;}$.\nLemma 20. Let $U, V$ be two smooth vector fields on a manifold $\\mathcal{M}$ equipped with a connection $\\nabla$. Further let $\\mathcal{U}$ be a neighborhood of $x \\in \\mathcal{M}$ such that $U| _{\\mathcal{u}} = g_1W_1 + \\cdots + g_n W_n$ for some $g_1, \\dots, g_n \\in \\mathfrak{F}(\\mathcal{U})$ and $W_1, \\dots, W_n \\in \\mathfrak{X}(\\upsilon)$. Then, $$(\\nabla_U V)(x) = g_1(x)(\\nabla_{W_1}V)(x) + \\cdots + g_n(x)(\\nabla_{W_n}V)(x),$$ where each vector $(\\nabla_{W_i}V)(x)$ is understood to mean $(\\nabla_{\\widetilde{W}_i}V)(x)$ with $\\widetilde{W}_i$ any smooth extension of $W_i$ to $\\mathcal{M}$ around $x$.\nDifferentiating Vector Fields on Curves Induced Covariant Derivative Definition 21. Let $c : I \\to \\mathcal{M}$ be a smooth curve on $\\mathcal{M}$ defined on an open interval $I$. A map $Z : I \\to \\operatorname{T} _{}{\\mathcal{M}}$ is a vector field on $c$ if $Z(t)$ is in $\\operatorname{T} _{c(t)}{\\mathcal{M}}$ for all $t \\in I$. Moreover, $Z$ is a smooth vector field on $c$ if it is also smooth as a map from $I$ to $\\operatorname{T} _{}{\\mathcal{M}}$. The set of smooth vector fields on c is denoted by $X(c)$.\nTheorem 22. Let $c \\colon I \\to M$ be a smooth curve on a manifold equipped with a connection $\\nabla$. There exists a unique operator $\\frac{D}{dt} \\colon \\mathfrak{X}(c) \\to \\mathfrak{X}(c)$ which satisfies the following properties for all $Y, Z \\in \\mathfrak{X}(c)$, $U \\in \\mathfrak{X}(\\mathcal{M})$, $g \\in \\mathfrak{F}(I)$, and $a, b \\in \\mathbb{R}$:\n$\\mathbb{R}$-linearity: $\\frac{D}{dt}(aY + bZ) = a\\frac{D}{dt}Y + b\\frac{D}{dt}Z$;\nLeibniz rule: $\\frac{D}{dt}(gZ) = \\frac{dg}{dt}Z + g\\frac{D}{dt}Z$;\nChain rule: $\\left(\\frac{D}{dt}(U \\circ c)\\right)(t) = \\nabla_{c\u0026rsquo;(t)}U$ for all $t \\in I$;\nProduct rule: If $M$ is a Riemannian manifold and $\\nabla$ is compatible with its metric (e.g., the Levi-Civita connection), then additionally: $$\\frac{d}{dt}\\langle Y, Z \\rangle = \\left\\langle \\frac{D}{dt}Y, Z \\right\\rangle + \\left\\langle Y, \\frac{D}{dt}Z \\right\\rangle$$ where the inner product $\\langle \\cdot, \\cdot \\rangle$ along $c$ is defined by $\\langle Y, Z \\rangle(t) = \\langle Y(t), Z(t) \\rangle _{c(t)}$.\nWe call $\\frac{D}{dt}$ the induced covariant derivative (induced by $\\nabla$).\nProposition 23. Let $\\mathcal{M}$ be an embedded submanifold of a Euclidean space $\\mathcal{E}$ with connection $\\nabla_{}{}$ as in Eq. 1. The operator $\\frac{D}{dt}$ defined by $$\\frac{D}{dt} Z(t) = \\operatorname{Proj}_{c(t)}{\\left(\\frac{\\text{d}{}}{\\text{d}{t}} Z(t)\\right)}$$ is the induced covariant derivative, that is, it satisfies properties 1\u0026ndash;3 in Theorem 22. If $\\mathcal{M}$ is a Riemannian submanifold of $\\mathcal{E}$, then $\\frac{D}{dt}$ also satisfies property 4 in that same theorem.\nAcceleration and Geodesics Definition 24. Let $c : I \\to \\mathcal{M}$ be a smooth curve. Its velocity is the vector field $c\u0026rsquo; \\in \\mathfrak{X}(c)$. The acceleration of $c$ is the smooth vector field $c\u0026rsquo;\u0026rsquo; \\in \\mathfrak{X}(c)$ defined by $c\u0026rsquo;\u0026rsquo; = \\frac{D}{dt} c\u0026rsquo;$. We also call $c\u0026rsquo;\u0026rsquo;$ the intrinsic acceleration of $c$.\nDefinition 25. On a Riemannian manifold $\\mathcal{M}$, a geodesic is a smooth curve $c : I \\to \\mathcal{M}$ such that $c\u0026rsquo;\u0026rsquo;(t) = 0$ for all $t \\in I$, where $I$ is an open interval of $\\mathbb{R}$.\nA Second-order Taylor Expansion on Curves Lemma 26. Let $c(t)$ be a geodesic connecting $x = c(0)$ to $y = c(1)$, and assume $\\operatorname{Hess}f(c(t)) \\succeq \\mu I$ for some $\\mu\\in \\mathbb{R}$ and all $t\\in [0,1]$. Then, $f(y) \\ge f(x) + \\langle \\operatorname{grad}f(x), v\\rangle _x + \\frac{\\mu}{2} \\parallel v \\parallel _x^2$.\nSecond-order Retractions Definition 27. A second-order retraction $R$ on a Riemannian manifold $\\mathcal{M}$ is a retraction such that, for all $x\\in \\mathcal{M}$ and all $v \\in \\operatorname{T}_{x}{\\mathcal{M}}$, the curve $c(t) = R_x(tv)$ has zero acceleration at $t = 0$, that is, $c\u0026rsquo;\u0026rsquo;(0) = 0$.\nProposition 28. Consider a Riemannian manifold $M$ equipped with any retraction $R$, and a smooth function $f: M \\to \\mathbb{R}$. If $x$ is a critical point of $f$ (that is, if $\\mathrm{grad}f(x) = 0$), then $$f(R_x(s)) = f(x) + \\frac{1}{2} \\langle \\mathrm{Hess}f(x)[s], s \\rangle _x + O(\\|s\\|_x^3).$$ Also, if $R$ is a second-order retraction, then for all points $x \\in M$ we have $$f(R_x(s)) = f(x) + \\langle \\mathrm{grad}f(x), s \\rangle _x + \\frac{1}{2} \\langle \\mathrm{Hess}f(x)[s], s \\rangle _x + O(\\|s\\|_x^3).$$Proposition 29. If the retraction is second order or if $\\operatorname{grad} f(x) = 0$, then $$\\operatorname{Hess} f(x) = \\operatorname{Hess} (f \\circ R_x)(0),$$ where the right-hand side is the Hessian of $f \\circ R_x : \\operatorname{T} _{x}{\\mathcal{M}} \\to \\mathbb{R}$ at $0 \\in \\operatorname{T} _{x}{\\mathcal{M}}$. The latter is a \u0026ldquo;classical\u0026rdquo; Hessian since $\\operatorname{T} _{x}{\\mathcal{M}}$ is a Euclidean space.\nRiemannian Submanifolds* Metric Projection Retractions* ","date":"2025-05-21T00:00:00Z","image":"http://localhost:1313/p/manifold-optimization-second-order-geometry-connections-and-hessians/manifold-retraction_hu_a7c2dd01cf7949e3.png","permalink":"http://localhost:1313/p/manifold-optimization-second-order-geometry-connections-and-hessians/","title":"Manifold Optimization - Second-Order Geometry: Connections and Hessians"},{"content":"","date":"2025-05-12T00:00:00Z","permalink":"http://localhost:1313/p/flow-method/","title":"Flow Method"},{"content":"","date":"2025-03-10T00:00:00Z","permalink":"http://localhost:1313/p/optimization-first-order-geometry-method/","title":"Optimization - First Order Geometry \u0026 Method"},{"content":"Introduction: A Proof of SMW Formula In the tutorial of MAT2041A Linear Algebra and Applications, I constructed a proof of the Sherman-Morrison-Woodbury Formula to demonstrate the potential of block matrices.\nSherman-Morrison-Woodbury Formula Sherman-Morrison-Woodbury Formula: Given $4$ matrices $\\textbf{A} \\in \\mathbb{R}^{p\\times p},\\textbf{U} \\in \\mathbb{R}^{p\\times q},\\textbf{V} \\in \\mathbb{R}^{q\\times p},\\textbf{C} \\in \\mathbb{R}^{q\\times q}$, where $\\textbf{A},\\textbf{C}, \\begin{bmatrix} \\textbf{A} \u0026amp; \\textbf{U} \\\\ \\textbf{V} \u0026amp; \\textbf{C} \\end{bmatrix}$ are invertible. Then,\n$$(\\textbf{A}+\\textbf{U}\\textbf{C}\\textbf{V})^{-1} = \\textbf{A}^{-1} - \\textbf{A}^{-1}\\textbf{U}(\\textbf{C}^{-1}+\\textbf{V}\\textbf{A}^{-1}\\textbf{U})^{-1}\\textbf{V}\\textbf{A}^{-1} \\stackrel{\\Delta}{=} f(\\textbf{A},\\textbf{C},\\textbf{U},\\textbf{V})$$ Proof We can observe that\n$$\\small\\begin{aligned}\\begin{bmatrix} f(\\textbf{A},\\textbf{C},\\textbf{U},\\textbf{V}) \u0026 \\textbf{O} \\\\\\\\ ? \u0026 \\textbf{I}_q \\end{bmatrix} \u0026= \\begin{bmatrix} \\textbf{I}_p \u0026 -\\textbf{A}^{-1}\\textbf{U} \\\\\\\\ \\textbf{O} \u0026 \\textbf{I}_q \\end{bmatrix} \\begin{bmatrix} \\textbf{I}_p \u0026 \\textbf{O} \\\\\\\\ \\textbf{O} \u0026 (\\textbf{C}^{-1}+\\textbf{V}\\textbf{A}^{-1}\\textbf{U})^{-1} \\end{bmatrix} \\begin{bmatrix} \\textbf{A}^{-1} \u0026 \\textbf{A}^{-1}\\textbf{U} \\\\\\\\ \\textbf{V}\\textbf{A}^{-1} \u0026 \\textbf{C}^{-1}+\\textbf{V}\\textbf{A}^{-1}\\textbf{U} \\end{bmatrix} \\\\\\\\ \u0026= \\begin{bmatrix} \\textbf{I}_p \u0026 -\\textbf{A}^{-1}\\textbf{U} \\\\\\\\ \\textbf{O} \u0026 \\textbf{I}_q \\end{bmatrix} \\begin{bmatrix} \\textbf{I}_p \u0026 \\textbf{O} \\\\\\\\ \\textbf{O} \u0026 (\\textbf{C}^{-1}+\\textbf{V}\\textbf{A}^{-1}\\textbf{U})^{-1} \\end{bmatrix} \\begin{bmatrix} \\textbf{A}^{-1} \u0026 \\textbf{O} \\\\\\\\ \\textbf{V}\\textbf{A}^{-1} \u0026 \\textbf{C}^{-1} \\end{bmatrix} \\begin{bmatrix} \\textbf{I}_p \u0026 \\textbf{U} \\\\\\\\ \\textbf{O} \u0026 \\textbf{I}_q \\end{bmatrix}\\end{aligned}$$Multiply the four block matrices on the right to $\\begin{bmatrix}\\textbf{A}+\\textbf{U}\\textbf{C}\\textbf{V} \u0026amp; \\textbf{O} \\\\ \\textbf{O} \u0026amp; \\textbf{I}_q\\end{bmatrix}$ and derive an identity matrix at top-left, which implies that $(\\textbf{A}+\\textbf{U}\\textbf{C}\\textbf{V})f(\\textbf{A},\\textbf{C},\\textbf{U},\\textbf{V}) = \\textbf{I}_p$.\n$\\square$\nComputation Let $\\textbf{D} \\stackrel{\\Delta}{=} \\textbf{C}^{-1}+\\textbf{V}\\textbf{A}^{-1}\\textbf{U}$, then\n$$\\small\\begin{aligned} \u0026\\begin{bmatrix} \\textbf{A} + \\textbf{U}\\textbf{C}\\textbf{V} \u0026 \\textbf{O} \\\\\\\\ \\textbf{O} \u0026 \\textbf{I} \\end{bmatrix} \\begin{bmatrix} \\textbf{I}_p \u0026 -\\textbf{A}^{-1}\\textbf{U} \\\\\\\\ \\textbf{O} \u0026 \\textbf{I}_q\\end{bmatrix} \\begin{bmatrix} \\textbf{I}_p \u0026 \\textbf{O} \\\\\\\\ \\textbf{O} \u0026 (\\textbf{C}^{-1}+\\textbf{V}\\textbf{A}^{-1}\\textbf{U})^{-1} \\end{bmatrix} \\begin{bmatrix} \\textbf{A}^{-1} \u0026 \\textbf{O} \\\\\\\\ \\textbf{V}\\textbf{A}^{-1} \u0026 \\textbf{C}^{-1} \\end{bmatrix} \\begin{bmatrix} \\textbf{I}_p \u0026 \\textbf{U} \\\\\\\\ \\textbf{O} \u0026 \\textbf{I}_q \\end{bmatrix} \\\\\\\\ =\u0026 \\begin{bmatrix}\\textbf{A}+\\textbf{U}\\textbf{C}\\textbf{V} \u0026 -\\textbf{U}-\\textbf{U}\\textbf{C}\\textbf{V}\\textbf{A}^{-1}\\textbf{U} \\\\\\\\ \\textbf{O} \u0026 \\textbf{I}_q\\end{bmatrix} \\begin{bmatrix}\\textbf{I}_p \u0026 \\textbf{O} \\\\\\\\ \\textbf{O} \u0026 \\textbf{D}^{-1} \\end{bmatrix} \\begin{bmatrix} \\textbf{A}^{-1} \u0026 \\textbf{O} \\\\\\\\ \\textbf{V}\\textbf{A}^{-1} \u0026 \\textbf{C}^{-1} \\end{bmatrix} \\begin{bmatrix} \\textbf{I}_p \u0026 \\textbf{U} \\\\\\\\ \\textbf{O} \u0026 \\textbf{I}_q \\end{bmatrix} \\\\\\\\ =\u0026 \\begin{bmatrix} \\textbf{A}+\\textbf{U}\\textbf{C}\\textbf{V} \u0026 -\\textbf{U}\\textbf{C}\\textbf{D}\\\\\\\\ \\textbf{O} \u0026 \\textbf{I}_q \\end{bmatrix} \\begin{bmatrix} \\textbf{I}_p \u0026 \\textbf{O} \\\\\\\\ \\textbf{O} \u0026 \\textbf{D}^{-1} \\end{bmatrix} \\begin{bmatrix} \\textbf{A}^{-1} \u0026 \\textbf{O} \\\\\\\\ \\textbf{V}\\textbf{A}^{-1} \u0026 \\textbf{C}^{-1} \\end{bmatrix} \\begin{bmatrix} \\textbf{I}_p \u0026 \\textbf{U} \\\\\\\\ \\textbf{O} \u0026 \\textbf{I}_q \\end{bmatrix} \\\\\\\\ =\u0026 \\begin{bmatrix} \\textbf{A}+\\textbf{U}\\textbf{C}\\textbf{V} \u0026 -\\textbf{U}\\textbf{C} \\\\\\\\ \\textbf{O} \u0026 \\textbf{D}^{-1} \\end{bmatrix} \\begin{bmatrix} \\textbf{A}^{-1} \u0026 \\textbf{O} \\\\\\\\ \\textbf{V}\\textbf{A}^{-1} \u0026 \\textbf{C}^{-1} \\end{bmatrix} \\begin{bmatrix} \\textbf{I}_p \u0026 \\textbf{U} \\\\\\\\ \\textbf{O} \u0026 \\textbf{I}_q \\end{bmatrix} \\\\\\\\ =\u0026 \\begin{bmatrix} \\textbf{I}_p \u0026 -\\textbf{U} \\\\\\\\ ? \u0026 ? \\end{bmatrix} \\begin{bmatrix} \\textbf{I}_p \u0026 \\textbf{U} \\\\\\\\\\textbf{O} \u0026 \\textbf{I}_q\\end{bmatrix} = \\begin{bmatrix} \\textbf{I}_p \u0026 \\textbf{O} \\\\\\\\? \u0026 ?\\end{bmatrix} \\\\\\\\ \\Rightarrow \u0026 (\\textbf{A}+\\textbf{U}\\textbf{C}\\textbf{V})f(\\textbf{A},\\textbf{C},\\textbf{U},\\textbf{V}) = \\textbf{I}_p \\end{aligned}$$Idea: Punching Hole Technique The \u0026ldquo;observed\u0026rdquo; block matrix multiplication actually creates identity matrices and zero matrices as sub-matrices. The related technique introduced by Hua Loo-Keng is called \u0026ldquo;Punching Hole\u0026rdquo; Principle (打洞原理 in Chinese) where the zero matrices are the holes in block matrices.\nRank of Block Matrices Guttman rank additivity formula: If a block matrix $\\mathbf{P}$ of $2\\times 2$ sub-matrices with left-top sub-matrix $\\mathbf{A}$ nonsingular, then\n$$\\small\\begin{aligned} \u003e\\text{rank}(\\mathbf{P}) = \\text{rank}(\\begin{bmatrix} \u003e\\mathbf{A} \u0026 \\mathbf{B} \\\\\\\\ \u003e\\mathbf{C} \u0026 \\mathbf{D} \u003e\\end{bmatrix}) = \\text{rank}(\\mathbf{A}) + \\text{rank}(\\mathbf{D} - \\mathbf{C}\\mathbf{A}^{-1}\\mathbf{B}) \u003e\\end{aligned}$$ where $\\mathbf{D} - \\mathbf{C}\\mathbf{A}^{-1}\\mathbf{B}$ is $\\mathbf{A}$\u0026rsquo;s Schur Complement, written as $\\mathbf{P} / \\mathbf{A}$.\nProof:\n$$\\small\\begin{aligned} \\text{rank}(\\begin{bmatrix} \\mathbf{A} \u0026 \\mathbf{B} \\\\\\\\ \\mathbf{C} \u0026 \\mathbf{D} \\end{bmatrix}) \u0026= \\text{rank}(\\begin{bmatrix} \\mathbf{I} \u0026 \\mathbf{O} \\\\\\\\ -\\mathbf{C}\\mathbf{A}^{-1} \u0026 \\mathbf{I} \\end{bmatrix} \\begin{bmatrix} \\mathbf{A} \u0026 \\mathbf{B} \\\\\\\\ \\mathbf{C} \u0026 \\mathbf{D} \\end{bmatrix} \\begin{bmatrix} \\mathbf{I} \u0026 -\\mathbf{A}^{-1}\\mathbf{B} \\\\\\\\ \\mathbf{O} \u0026 \\mathbf{I} \\end{bmatrix}) \\\\\\\\ \u0026= \\text{rank}(\\begin{bmatrix} \\mathbf{A} \u0026 \\mathbf{B} \\\\\\\\ \\mathbf{O} \u0026 \\mathbf{D} - \\mathbf{C}\\mathbf{A}^{-1}\\mathbf{B} \\end{bmatrix} \\begin{bmatrix} \\mathbf{I} \u0026 -\\mathbf{A}^{-1}\\mathbf{B} \\\\\\\\ \\mathbf{O} \u0026 \\mathbf{I} \\end{bmatrix}) \\\\\\\\ \u0026= \\text{rank}(\\begin{bmatrix} \\mathbf{A} \u0026 \\mathbf{O} \\\\\\\\ \\mathbf{O} \u0026 \\mathbf{D} - \\mathbf{C}\\mathbf{A}^{-1}\\mathbf{B} \\end{bmatrix}) = \\text{rank}(\\mathbf{A}) + \\text{rank}(\\mathbf{D} - \\mathbf{C}\\mathbf{A}^{-1}\\mathbf{B}) \\end{aligned}$$$\\square$\nDeterminant of Block Matrices Extension: Block Matrix Rank Inequality Using the Punching Hole Technique above, we can derive the following rank inequalities.\nSylvester Inequality Sylvester Inequality: For any matrices $\\mathbf{A} \\in \\mathbb{R}^{m\\times n}, \\mathbf{B} \\in \\mathbb{R}^{n\\times k}$, then\n$$\\text{rank}(\\mathbf{AB}) \\ge \\text{rank}(\\mathbf{A}) + \\text{rank}(\\mathbf{B}) - n$$ Proof: By Guttman rank additivity formula with $\\mathbf{A} = \\mathbf{I}_n$, then\n$$\\small \\begin{aligned}\\text{rank}(\\begin{bmatrix} \\mathbf{I}_n \u0026 \\mathbf{B} \\\\\\\\ \\mathbf{A} \u0026 \\mathbf{O}_{m\\times k}\\end{bmatrix}) \u0026= \\text{rank}(\\begin{bmatrix} \\mathbf{I}_{n} \u0026 \\mathbf{O} \\\\\\\\ -\\mathbf{A} \u0026 \\mathbf{I}_{m}\\end{bmatrix} \\begin{bmatrix} \\mathbf{I}_n \u0026 \\mathbf{B} \\\\\\\\ \\mathbf{A} \u0026 \\mathbf{O}_{m\\times k}\\end{bmatrix} \\begin{bmatrix} \\mathbf{I}_{n} \u0026 -\\mathbf{B} \\\\\\\\ \\mathbf{O} \u0026 \\mathbf{I}_{k}\\end{bmatrix}) \\\\\\\\ \u0026= \\text{rank}(\\begin{bmatrix} \\mathbf{I} \u0026 \\mathbf{O}_{n\\times k} \\\\\\\\ \\mathbf{O}_{m\\times n} \u0026 -\\mathbf{AB} \\end{bmatrix}) = \\text{rank}(\\mathbf{I}_n) + \\text{rank}(\\mathbf{AB}) = n + \\text{rank}(\\mathbf{AB}) \\end{aligned}$$Therefore,\n$$\\text{rank}(\\mathbf{AB}) = \\text{rank}(\\begin{bmatrix} \\mathbf{I}_n \u0026 \\mathbf{B} \\\\\\\\ \\mathbf{A} \u0026 \\mathbf{O}_{m\\times k}\\end{bmatrix}) - n \\ge \\text{rank}(\\mathbf{A}) + \\text{rank}(\\mathbf{B}) - n$$$\\square$\nFrobenius Inequality Frobenius Inequality: For any matrices $\\mathbf{A} \\in \\mathbb{R}^{m\\times n}, \\mathbf{B} \\in \\mathbb{R}^{n\\times k}, \\mathbf{C} \\in \\mathbb{R}^{k\\times l}$, then\n$$\\text{rank}(\\mathbf{AB}) + \\text{rank}(\\mathbf{BC}) \\le \\text{rank}(\\mathbf{ABC}) + \\text{rank}(\\mathbf{B})$$ Proof: Similar with Guttman rank additivity formula (but note that we don\u0026rsquo;t have inverse matrix of $\\mathbf{A}$),\n$$\\small \\begin{aligned}\\text{rank}(\\begin{bmatrix} \\mathbf{B} \u0026 \\mathbf{BC} \\\\\\\\ \\mathbf{AB} \u0026 \\mathbf{O}_{m\\times l}\\end{bmatrix}) \u0026= \\text{rank}(\\begin{bmatrix} \\mathbf{I}_{n} \u0026 \\mathbf{O} \\\\\\\\ -\\mathbf{A} \u0026 \\mathbf{I}_{m}\\end{bmatrix}\\begin{bmatrix} \\mathbf{B} \u0026 \\mathbf{BC} \\\\\\\\ \\mathbf{AB} \u0026 \\mathbf{O}_{m\\times l}\\end{bmatrix} \\begin{bmatrix} \\mathbf{I}_{k} \u0026 -\\mathbf{C} \\\\\\\\ \\mathbf{O} \u0026 \\mathbf{I}_{l}\\end{bmatrix}) = \\text{rank}(\\begin{bmatrix} \\mathbf{B} \u0026 \\mathbf{O}_{n\\times l} \\\\\\\\ \\mathbf{O}_{m\\times k} \u0026 -\\mathbf{ABC} \\end{bmatrix}) = \\text{rank}(\\mathbf{B}) + \\text{rank}(\\mathbf{ABC}) \\end{aligned}$$Therefore,\n$$\\text{rank}(\\mathbf{AB}) + \\text{rank}(\\mathbf{BC}) = \\text{rank}(\\begin{bmatrix} \\mathbf{O}_{n\\times k} \u0026 \\mathbf{BC} \\\\\\\\ \\mathbf{AB} \u0026 \\mathbf{O}_{m\\times l}\\end{bmatrix}) \\le \\text{rank}(\\begin{bmatrix} \\mathbf{B} \u0026 \\mathbf{BC} \\\\\\\\ \\mathbf{AB} \u0026 \\mathbf{O}_{m\\times l}\\end{bmatrix}) = \\text{rank}(\\mathbf{ABC}) + \\text{rank}(\\mathbf{B})$$$\\square$\n","date":"2025-03-08T00:00:00Z","image":"http://localhost:1313/p/linear-algebra-punching-hole-technique-on-matrices/SMW-slide_hu_6e70bc4efc46de1c.jpg","permalink":"http://localhost:1313/p/linear-algebra-punching-hole-technique-on-matrices/","title":"Linear Algebra - Punching Hole Technique on Matrices"}]